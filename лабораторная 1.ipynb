{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вариант 3\n",
    "\n",
    "Провести морфологический анализ текста с простейшим снятием омонимии. Организовать хранение результатов разбора в префиксном дереве, при этом в качестве первого ключа используется часть речи, а в качестве следующих — грамматические параметры слов. Слова с совпадающей частью речи и грамматическими параметрами хранить в отсортированном списке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2 as pm2\n",
    "from re import sub\n",
    "\n",
    "morph = pm2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text): \n",
    "    \n",
    "    cleantext = ''\n",
    "    with open(text,'r',encoding='utf-8') as f:\n",
    "    # Чистим текст от знаков препинания и некириллических символов \n",
    "        words = f.read()\n",
    "        for i in words:\n",
    "            if i in '''ЙЦУКЕНГШЩЗХЪЭЖДЛОРПАВЫФЯЧСМИТЬБЮЁ\n",
    "                    йцукенгшщзхъэждлорпавыфячсмитьбюё\n",
    "                    1234567890- ''':\n",
    "                cleantext += i\n",
    "    # Заменяем пробелоподобные символы на пробелы\n",
    "    cleantext = sub(r'\\s',' ',cleantext)\n",
    "    cleantext = cleantext.split()\n",
    "   \n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(text):\n",
    "    \n",
    "    an = [] # общий список разборов\n",
    "    for word in text:\n",
    "        scores = []\n",
    "        # Парсим слово с помощью pymorphy2\n",
    "        p = morph.parse(word)\n",
    "        # Снимаем омонимию\n",
    "        # Выбираем самую вероятную трактовку благодаря параметру score\n",
    "        scores.append(i.score for i in p)\n",
    "        n = scores.index(max(scores)) \n",
    "        p = morph.parse(word)[n]\n",
    "        # Добавляем самый вероятный вариант разбора в общий список разборов всех слов\n",
    "        an.append(p.tag)\n",
    "    \n",
    "    return an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareanalysis(an): \n",
    "    # Для каждого слова анализ сейчас выглядит примерно так: 'VERB,impf,tran plur,3per,pres,indc'\n",
    "    # нам надо сделать из этого список\n",
    "    \n",
    "    listofgr = [] \n",
    "    for gr in an:\n",
    "        gr = str(gr)\n",
    "        gr = gr.split(',') # делим элементы по запятой, однако и сейчас у нас есть значения вроде [tran plur]\n",
    "                           # мы хотим поделить их на 2 отдельных элемента\n",
    "        for i in range(len(gr)):\n",
    "            gr[i] = gr[i].split() # у нас оказались вложенные списки\n",
    "        listofgr.append([i for sublist in gr for i in sublist]) # делаем одномерный список\n",
    "                                                                # теперь каждый анализ будет выглядеть примерно так:\n",
    "                                                                # ['VERB','impf','tran','plur','3per','pres','indc']\n",
    "    \n",
    "    return listofgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPostfix(postfix, curLevel):\n",
    "    \n",
    "    # Если в префиксе остался только один элемент, то это наше слово\n",
    "    if len(postfix) == 1:  \n",
    "        # Добавим словарь {'слова':[слово 1, слово 2, слово 3...]} с элеменатми с одинаковыми признаками в дерево\n",
    "        if curLevel.get('слова', None) == None:\n",
    "            curLevel.update({'слова': [postfix[0]]})\n",
    "        else:\n",
    "            # Не будем добавлять одно слово много раз, чтобы не загружать дерево\n",
    "            if postfix[0] not in curLevel['слова']:\n",
    "                curLevel['слова'].append(postfix[0])\n",
    "                curLevel['слова'] = sorted(curLevel['слова'])\n",
    "        return\n",
    "    # Такого начала нет в словаре. Его надо добавить.\n",
    "    if curLevel.get(postfix[0], None) == None:\n",
    "        curLevel[postfix[0]] = {}\n",
    "    # Добавляем оставшуюся часть в соответствующий узел.\n",
    "    addPostfix(postfix[1:], curLevel[postfix[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'input.txt'\n",
    "words = cleaning(text)\n",
    "analysis = parsing(words)\n",
    "properanalysis = prepareanalysis(analysis)\n",
    "trie = {}\n",
    "for i in range(len(properanalysis)):\n",
    "    properanalysis[i].append(words[i].lower()) # добавляем в конец каждого списка само слово, чтобы внести его в дерево\n",
    "    addPostfix(properanalysis[i], trie)\n",
    "\n",
    "#print(trie)\n",
    "#print(trie.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
